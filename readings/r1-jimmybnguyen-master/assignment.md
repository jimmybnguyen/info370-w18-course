# r1-case-studies

## Overview
The purpose of this assignment is to use case studies to understand and assess how researchers map from abstract concepts to the data structures necessary for analysis. These papers are **short, but dense**. You will be responsible for understanding the details of their procedures, but _do not necessarily_ need to understand the statistical methods employed. In order to appropriately answer the questions below, you will likely need to **read the papers multiple times**. I suggest that you skim each paper to get a broad understanding, then **re-read the paper** to understand the details. Attempting to answer the questions below without reading the entire paper will both be less accurate and take more time.

## Detecting influenza epidemics using search engine query data

1. What is the purpose of this research? _(1 paragraph, 3 points)_
> This research explored how we can improve early detection of seasonal influenza epidemics by monitoring online search queries. The researchers analyzed a large number of Google search queries to track illness similar to influenza in a population. Since the relative frequency of certain queries has a high correlation with the percentage of physician visits where the patients with symptoms for influenza, they were able to estimate the current level of weekly influenza activity in each region of the United States. 

2. What are current approaches to detecting influenza outbreaks, and what are their limitations? _(1 paragraph, 3 points)_
> Traditional influenza surveillance systems depended on virological and clinical data, such as influenza-like illness physician visits. The drawback with this is that the Centers for Disease Control and Prevention (CDC) has a 1-2 week reporting lag with their weekly national and regional data from these surveillance systems. I imagine the long reporting lag is due to a large amount of manual work for these weekly reports. 

3. The title of the paper introduces two broad concepts: _influenza epidemics_ and _search engine queries_. In order to assess the relationship between these two concepts, each one needs to be mapped to a **data structure**. For each concept, answer the following questions: _(~2 paragraphs, 10 points)_

    a. What were the sources of data? Once the data is formatted for analysis, what does a single observation in the (prepared) data represent (think about the _unit of analysis_)? _(5 points)_
    > The main source of data came from 5 years of Google web search logs between 2003 and 2008 with both regional and state-level estimates of influenza-like illness (ILI) activities in the United States. Historical data from the CDC’s US Influenza Sentinel Provider Surveillance Network was used to help build the researcher’s model, which estimated the probability that a random physician visit in a region is related to an ILI. Once the data was formatted for analysis, a single observation in the prepared data represented a ILI-related search query in a particular region. This data point is then measured against the CDC ILI data to see how it would fit using the researcher’s model. 

    b. What concerns do you have about the relationship between the abstract concepts and the data used to represent them in the analysis? Do you feel as if the data adequately captures the concepts? Why/why not? _(5 points)_
    > The main concern I have about the relationship between the abstract concepts and the data used to represent them is how exactly the automated method of selecting ILI-related search queries worked. From the methods summary, it looks like they chose to only look at search queries that contained no misspellings and the word “flu.” There are other words that people can use to search for flu symptoms, such as words in a different language. There may be a significant number of queries that contained misspellings. Overall, I feel that the data adequately captures the concepts because a majority of queries with the word “flu” during flu season most likely came from people who are experiencing symptoms. 

4. In the primary statistical analysis, what are the **null and alternate hypothesis**? _(1 paragraph, 4 points)_
> The null hypothesis in the primary statistical analysis is that there no relationship between the number of certain search queries with influenza-like keywords in a region and influenza-like illness percentage in that particular region. The alternative hypothesis is that there is a relationship between the number of certain search queries with influenza-like keywords in a region and influenza-like illness percentage in that particular region.

5. Both variables used in the study had to be _normalized_ at some point in the analytical process. What does _normalization_ involve, and why was it important for this study? _(1 paragraph, 3 points)_
> Normalizing variables in a study involves ensuring that the variables are under the same unit of measure so that they are comparable to each other. In this case, it helped find a relationship between the percentage of online search queries in a given region that contained ILI keywords and the random probability of a physician visits where patients have influenza-like symptoms. 

6. No data was provided outside the flu season -- do you think this has any implication for the analysis? Why / why not? _(1 paragraph, 4 points)_
> I believe this may have implication for the analysis because people’s search habits for influenza-like illnesses may be different during flu season and outside flu season. For example, during flu season, people may be more likely to search for influenza-like symptoms because they are aware they the flu is more common during this time. Since no data was provided outside the flu season, I believe the findings from this research cannot be applied to detecting influenza epidemics outside of the flu season because it’s a different situation with potentially different variables to control. 

7. The paper described a method for assessing (validating) their statistical model. What did they do, and what were the results? _(1 paragraph, 5 points)_.
> The researchers developed an automated method of selecting ILI-related search queries, which resulted in about 50 million candidate queries. Each query was measured to see which ones could most accurately represent the CDC ILI visit percentage in each region. The automated process made a list of the highest scoring search queries. The researchers considered different sets of top-scoring queries to see which one would be influenced in the ILI-related query fraction. They found that combining the 45 highest-scoring queries gave the best fit with the CDC ILI data. 

8. Identify a topic/skill in the paper that interests you (statistical method, etc.). Then, do a bit of research, and report back. Feel free to show a sample demonstration of this method in a language of your choice, or a layman's interpretation of what they method does _(~2 paragraphs, 8 points)_.
> The researchers used a cross-validation to validate their model that estimates how many random physician visit in a particular region are related to an ILI, to regional data from the CDC. Cross-validation is a model validation technique that is used to estimate how accurate a predictive model will be in practice. Cross validation also helps with solving the issue of overfitting a model.

> Overfitting occurs when a model fits too closely with a particular set of data. As such, it may fail to predict future observations in a reliable manner. Cross-validation prevents overfitting by testing a model on a set of data that is not used in estimation, also known as the “test set.” The data used for estimation is called the “training set.” The end goal is to gain insight in how the model will generalize to an independent dataset. The researchers did four cross-validations test per region in the United States to obtain many points of assurance that their model can reliably predict influenza epidemics. 


9. In layman's terms, what does the linear regression attempt to measure? _(1 paragraph, 4 points)_
> The linear regression attempted to measure how well search queries with ILI-related keywords for all nine regions of the United States matched the CDC ILI regional data for the percentage of physician visits where the patients had influenza-like symptoms. With an average correlation of 90%, the ILI-related search queries had a good fit with the regional data from the CDC. 

10. Overall, what is your impression of this study? Do you believe the insights are reliable and can be applied in real world situations? _(1 paragraph, 4 points)_
> With the high correlation between the researcher’s linear model and the regional data from the CDC, I believe the findings from this research could be used to detect influenza epidemics during the flu season, as long as the data from the CDC is an accurate representation of the US. The researchers placed careful thought in how they were going to find the best fit between their data and the CDC, and validated the model on different variables and scale (region vs. states). I believe the methods in this paper can be applied to future research to detect other epidemics or phenomenons.

## Experimental evidence of mass-scale emotional contagion through social networks

1. What is the purpose of this research? _(1 paragraph, 3 points)_
> The purpose of this research is to conclude whether emotional contagion can occur outside of an in-person interaction. Specifically, the researchers looked at how the omitting certain content of someone’s Feed could their emotions in future posts. A similar 20 year long research conducted before came under scrutiny due to its correlational nature. An experimental study was done to address this, but the methods used were criticized for looking at emotions after social interactions, which suggest that emotion contagion may come from experiencing an interaction *rather* than exposure to another person’s emotion. This research sought to address the concerns of previous study to come up with a solid conclusion on emotional contagion outside of in-person interactions.

2. Unlike the above study, which was _observational_, this study conducted an _experiment_. Describe, in detail, how the experiment was conducted. Be sure to mention **who** was included in the study, **when** it was conducted, and **how** individuals in the study were _experimented upon_. _(2 paragraphs, 6 points)_
> The experiment lasted for one week on the days of January 11-18, 2012. The participants of the study were randomly selected based on their Facebook User ID, must had posted at least one status update during the experimental period, and viewed Facebook in English. This resulted in about 155,000 participants. Prior to creating a Facebook account, users agreed to Facebook’s Data Use Policy, which supposedly resulted in informed consent for this research. 


Two parallel experiments were conducted for positive and negative emotions. In one experiment, exposure to positive emotions were reduced. In the other experiment, negative emotions were reduced. This was done by omitting posts that had emotional content of the relevant emotion being tested. Each emotional post had between a 10% and 90% chance of being omitted from the News Feed for any specific viewing, which depended on the User ID. Both experiments had a controlled condition, “in which a similar proportion of posts in their News Feed were omitted entirely at random.”


3. In order to conduct this study, the researchers had to map from the concepts of emotional positivity/negativity to a dataset. How were these concepts captured in the data, and what are some of the limitations of this approach? _(1 paragraph, 5 points)_
> The concepts of emotional positivity/negativity were captured in the data by looking to see if the posts contained positive or negative words as defined by Linguistic Inquiry and Word Count software word counting system. From there, the researchers were able to see what portions of posts in the experimental and control trials contained positive and negative words. One limitation of this approach is that the software used may have not recognized some words to be positive or negative due to slang. Similarly, some people may had used a positive word in a negative manner and vice versa. It is easy to see what words were used in these social posts, but it is difficult to see the *intent* of them. 

4. Using at least one outside reference, identify and describe an alternative approach for determining the emotional content of a piece of text. _(1 paragraph, 4 points)_
> Research conducted by Cecilia Alm et al. explored how to use supervised machine learning to predict emotional content of a piece of text. The main goal of the research was to classify sentences in children fairy tales with emotions because they are an important part of the story plot. They implemented a SNoW (Sparse Network of Winnows) learning architecture to annotate 185 children stories with emotions. The algorithm determines the emotion of a linguistic unit in a sentence and annotated with with the appropriate emotion. The annotations can be used to create an appropriate tone for text-to-speech software. 


> *Alm, C. O., D. Roth and R. Sproat. 2005. Emotions from Text: Machine Learning for Text-based Emotion Prediction. In Proceedings of the Human Language Technology Conference and the 2005 Conference on Empirical Methods in Natural Language Processing, Vancouver, Canada, 6-8 October, pp. 579-586.*


5. The statistical analyses presented in the paper report _statistical significance_ with a p-value < .05 (more on this later in the course). However, are their observations _practically significant_ (i.e. meaningful)? How do the authors defend the claim that their observations are socially consequential? _(1 paragraph, 4 points)_
> The authors defended the claim that their observation are socially consequential by stating that small effects can have huge consequences for social networks as big as Facebook. Even with an effect size of d = 0.001, this would correspond to hundred of thousands of emotions expressions in News Feed posts in early 2013. They also claim that their observations are consequential because the manipulation of the independent variable (emotion being present in the News Feed) was minimal, and the dependent variable (emotional expressions by people) was hard to manipulate due to the range of daily experiences that may influence mood. For a majority of users, the findings in this paper may not be practically significant, especially to people who do not frequent post, but I do agree that even a small effect size has significant impact on a user base as big as Facebook. 

6. Identify a topic/skill in the paper that interests you (statistical method, etc.). Then, do a bit of research, and report back. Feel free to show a sample demonstration of this method in a language of your choice, or a layman's interpretation of what they method does. _(~2 paragraphs, 8 points)_
>The statistical method I will focus found in this paper is least squared method. This method is a form of mathematical regression analysis, which is a set of statistical processes for estimating relationships among variables, that finds the line of best fit for a dataset. The line of best fit is a straight line that is the best representation of a given data set, used to study the relationship between two variables. In this research paper, the researchers did a weighted linear regression to predict the percentage of words that were positive or negative for a particular condition.

>A weighted linear regression is when a data item is given more emphasis than other items in a data set. In this case, the researchers weighted the data by the likelihood of a person having an emotional post omitted from their News Feed for a particular viewing. This resulted in people who had more content omitted to have more emphasis in the linear regression. 


7. What are some of the ethical concerns of conducting such a study? _(1 paragraph, 4 points)_
> The main ethical concern of conducting this study is how the participants were sourced. According to the researchers, this study was consistent with Facebook’s Data Use Policy and participants gave informed consent when agreeing to this policy when creating a account. However, the Facebook Data Use Policy was mostly likely extremely long and dense to read. A majority of users most likely did not read this policy when creating an account because they did not think about the consequences, which means they were not appropriately informed. One may argue that people should read the policy before agreeing to it, but I would argue that Facebook should had done a better job in informing people how their data might be used, and should people more control in this aspect. 

8. Like many studies that seek to measure the effect in an experiment, this was a _case-control_ study. What was the _control_ group, and was it important to include it (why/why not)? _(1 paragraph, 4 points)_
> Both of the experiments in this study had a control group in which a similar portion of posts in people’s News Feed were omitted entirely at random. This was important to include because there were less posts that contained negative words and more posts that contained positive words. By randomly omitting posts, the researchers were able to eliminate variables that may have an impact on the final conclusions and solely focus on one variable to rest. 

9. The presumed _causal pathway_ of this study is that seeing an emotional post _leads to_ a changed emotional state _resulting in_ changes in social media behavior. What are **alternative causal pathways** that explain the relationship between the presumed cause (change in content) and resulting behavior (change in posting). _(1 paragraph, 5 points)_
> One alternative casual pathway to explain this relationship are external factors outside the social media world. People who have social media accounts have their own personal lives that are affected by friends, family, work, etc. There may be emotional contagion happening from these external, in-person factors that changed their behavior online. Another casual pathway may include other content on the News Feed, such as Facebook Ads. It was also not clear if videos or other multimedia content were a part of this study, which may also play a role. 

10. The study describes a linear regression as it's primary assessment. Answer the following questions regarding the analysis: _(2 paragraphs, 4 points)_

    a. What was the unit of analysis (i.e., what would each row of data represent)? _(2 points)_
    > The unit of analysis in this study is the percentage of positive or negative words in a given post. This is because the researchers were looking at the increase or decrease in positive or negative posts from people who had content certain omitted from their News Feed. 

    b. Explain (intuitively) what this analysis is attempting to measure. _(2 points)_
    > The analysis is attempting to measure the effect of reducing negative or positive posts in the two groups being experimented on. The analysis found that people who had positive posts reduced in their News Feed were less positive overall than the control group where post were omitted at random. Similarly, they found that people who had negative posts reduced in their News Feed were less negative overall than the control group. 

11. Overall, what is your impression of this study? Do you believe the insights are reliable and can be applied in real world situations? _(1 paragraph, 5 points)_
> While I am not too fond on how Facebook got “informed” participants for this study, I do believe that the findings are reliable. It further proves the power that social media platforms can have over our lives. By simply tweaking the News Feed algorithm, Facebook can shift the overall mood of Facebook users towards a direction they feel like would benefit them the most monetarily at any point. Other real world situations I believe this may apply to are other text-based communication methods such as Facebook Messenger. The words sent on Messenger, and other instant messaging platform, are more personal by nature because instead of broadcasting to a wide audience, people are talking in small groups or 1:1. This may result in greater emotional contagion.
